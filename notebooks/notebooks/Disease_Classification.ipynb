{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778a7d69-1b1c-432b-a65d-fccfd386f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9e7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a0122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\parvathi143\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import os                       # for working with files\n",
    "import numpy as np              # for numerical computationss\n",
    "import pandas as pd             # for working with dataframes\n",
    "import torch                    # Pytorch module \n",
    "import matplotlib.pyplot as plt # for plotting informations on graph and images using tensors\n",
    "import torch.nn as nn           # for creating  neural networks\n",
    "from torch.utils.data import DataLoader # for dataloaders \n",
    "from PIL import Image           # for checking images\n",
    "import torch.nn.functional as F # for functions for calculating loss\n",
    "import torchvision.transforms as transforms   # for transforming images into tensors \n",
    "from torchvision.utils import make_grid       # for data checking\n",
    "from torchvision.datasets import ImageFolder  # for working with classes and images\n",
    "from torchsummary import summary              # for getting the summary of our model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e297f5-3e46-4208-83d6-58398fc45927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found Diseases: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___healthy', 'Corn_(maize)___Northern_Leaf_Blight', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___healthy', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = r\"C:\\Users\\parvathi143\\Desktop\\webapp\\webapp\\Dataset_collection\\New Plant Diseases Dataset(Augmented)\"\n",
    "train_dir = os.path.join(data_dir, \"train\")  # Automatically creates full path\n",
    "valid_dir = os.path.join(data_dir, \"valid\")  \n",
    "\n",
    "# Check if the train directory exists\n",
    "if os.path.exists(train_dir):\n",
    "    diseases = os.listdir(train_dir)  # List disease folders\n",
    "    print(\"üìÇ Found Diseases:\", diseases)\n",
    "else:\n",
    "    print(\"‚ùå Error: Training directory not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551a6905-fe8a-4e83-aa26-82e269f4e01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___healthy', 'Corn_(maize)___Northern_Leaf_Blight', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___healthy', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot']\n"
     ]
    }
   ],
   "source": [
    "# printing the disease names\n",
    "print(diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90730367-5e19-4feb-87c4-a0fd83662ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total disease classes are: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Total disease classes are: {}\".format(len(diseases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4a34f0-aa52-4b36-86a6-1937fd39dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plants = []\n",
    "NumberOfDiseases = 0\n",
    "for plant in diseases:\n",
    "    if plant.split('___')[0] not in plants:\n",
    "        plants.append(plant.split('___')[0])\n",
    "    if plant.split('___')[1] != 'healthy':\n",
    "        NumberOfDiseases += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf3b161-014c-4922-838a-b428f93a2b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Plants are: \n",
      "['Apple', 'Blueberry', 'Cherry_(including_sour)', 'Corn_(maize)', 'Grape', 'Orange', 'Peach', 'Pepper,_bell']\n"
     ]
    }
   ],
   "source": [
    "# unique plants in the dataset\n",
    "print(f\"Unique Plants are: \\n{plants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11247ce6-1e8c-46a7-ad29-2d0981fee788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of plants: 8\n"
     ]
    }
   ],
   "source": [
    "# number of unique plants\n",
    "print(\"Number of plants: {}\".format(len(plants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a2bdab-aa95-4f5b-a096-1c376c11d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of diseases: 13\n"
     ]
    }
   ],
   "source": [
    "# number of unique diseases\n",
    "print(\"Number of diseases: {}\".format(NumberOfDiseases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4f4fc2-2df9-4ed2-af6d-513605798b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no. of images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple___Apple_scab</th>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple___Black_rot</th>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple___Cedar_apple_rust</th>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple___healthy</th>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blueberry___healthy</th>\n",
       "      <td>1816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cherry_(including_sour)___healthy</th>\n",
       "      <td>1826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cherry_(including_sour)___Powdery_mildew</th>\n",
       "      <td>1683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot</th>\n",
       "      <td>1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corn_(maize)___Common_rust_</th>\n",
       "      <td>1907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corn_(maize)___healthy</th>\n",
       "      <td>1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corn_(maize)___Northern_Leaf_Blight</th>\n",
       "      <td>1908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grape___Black_rot</th>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grape___Esca_(Black_Measles)</th>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grape___healthy</th>\n",
       "      <td>1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grape___Leaf_blight_(Isariopsis_Leaf_Spot)</th>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orange___Haunglongbing_(Citrus_greening)</th>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peach___Bacterial_spot</th>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peach___healthy</th>\n",
       "      <td>1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pepper,_bell___Bacterial_spot</th>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    no. of images\n",
       "Apple___Apple_scab                                           2016\n",
       "Apple___Black_rot                                            1987\n",
       "Apple___Cedar_apple_rust                                     1760\n",
       "Apple___healthy                                              2008\n",
       "Blueberry___healthy                                          1816\n",
       "Cherry_(including_sour)___healthy                            1826\n",
       "Cherry_(including_sour)___Powdery_mildew                     1683\n",
       "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot           1642\n",
       "Corn_(maize)___Common_rust_                                  1907\n",
       "Corn_(maize)___healthy                                       1859\n",
       "Corn_(maize)___Northern_Leaf_Blight                          1908\n",
       "Grape___Black_rot                                            1888\n",
       "Grape___Esca_(Black_Measles)                                 1920\n",
       "Grape___healthy                                              1692\n",
       "Grape___Leaf_blight_(Isariopsis_Leaf_Spot)                   1722\n",
       "Orange___Haunglongbing_(Citrus_greening)                     2010\n",
       "Peach___Bacterial_spot                                       1838\n",
       "Peach___healthy                                              1728\n",
       "Pepper,_bell___Bacterial_spot                                1913"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of images for each disease\n",
    "nums = {}\n",
    "for disease in diseases:\n",
    "    nums[disease] = len(os.listdir(train_dir + '/' + disease))\n",
    "    \n",
    "# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n",
    "\n",
    "img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\n",
    "img_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84e8e03-e9e5-49d4-aae7-dd985e0cc58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35123 images for training\n"
     ]
    }
   ],
   "source": [
    "n_train = 0\n",
    "for value in nums.values():\n",
    "    n_train += value\n",
    "print(f\"There are {n_train} images for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c09b52a9-9eb0-4cba-a008-b3ddd04da170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# datasets for validation and training\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = r\"C:\\Users\\parvathi143\\Desktop\\webapp\\webapp\\Dataset_collection\\New Plant Diseases Dataset(Augmented)\\train\"\n",
    "valid_dir = r\"C:\\Users\\parvathi143\\Desktop\\webapp\\webapp\\Dataset_collection\\New Plant Diseases Dataset(Augmented)\\valid\"\n",
    "\n",
    "# ‚úÖ Transform the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # Resize images to 150x150\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch Tensors\n",
    "])\n",
    "\n",
    "# ‚úÖ Load datasets with ImageFolder\n",
    "train = ImageFolder(train_dir, transform=transform)\n",
    "valid = ImageFolder(valid_dir, transform=transform)\n",
    "\n",
    "print(\"‚úÖ Datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bec4806-1cd8-49d9-b7ea-945a51045cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 150, 150]) 0\n"
     ]
    }
   ],
   "source": [
    "img, label = train[0]\n",
    "print(img.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94dbf49d-510b-4cea-8e71-692d765ebcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of classes in train set\n",
    "len(train.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5051cb0-5f22-4dfa-bb0b-e7974f72573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking some images from training dataset\n",
    "def show_image(image, label):\n",
    "    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n",
    "    plt.imshow(image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b1a63a-2bd7-4c29-a430-7d5782358c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_image(*train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df88629-41e4-413c-81c9-0e7102ef3890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5140b94-c916-4d1a-a91a-15312a4afa0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2aea57d4e70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17c24d89-b3f9-4729-9449-cc3fdeadcb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07bea3ef-95b8-4020-af8f-d4f767acbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders for training and validation\n",
    "#train_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "#valid_dl = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbeabd82-d650-4476-9e89-72b079279cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to show a batch of training instances\n",
    "def show_batch(data):\n",
    "    for images, labels in data:\n",
    "        fig, ax = plt.subplots(figsize=(30, 30))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a2f3a95-9392-442d-b0a9-23577c1f1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69c9e81d-f980-4956-b01d-a561936c3ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "187998ba-20d9-48a7-bbb5-8449a7c35cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# For moving data to the selected device (CPU or GPU)\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to the chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]  # Recursively move lists/tuples of data\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "# Wraps a dataloader to automatically move data to the device (CPU or GPU)\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to the device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e0f5b58-d9c7-4717-aa0f-156b93902849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3065f54d-e66b-45d1-97cc-0a4362005ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dl = DeviceDataLoader(train_dl, device)\n",
    "#valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7256aeff-8114-4279-a959-969916072e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleResidualBlock(\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # ‚úÖ Importing nn module\n",
    "\n",
    "class SimpleResidualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # ‚úÖ Correct indentation\n",
    "        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual  # Add residual connection\n",
    "\n",
    "# ‚úÖ Test the block\n",
    "block = SimpleResidualBlock()\n",
    "print(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b0d876e-e100-40ca-978d-63afd3ba9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating the accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "# base class for the model\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                   # Generate prediction\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)          # Calculate accuracy\n",
    "        return {\"val_loss\": loss.detach(), \"val_accuracy\": acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()       # Combine loss  \n",
    "        epoch_accuracy = torch.stack(batch_accuracy).mean()\n",
    "        return {\"val_loss\": epoch_loss, \"val_accuracy\": epoch_accuracy} # Combine accuracies\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e747ef33-ad55-4643-9926-cfc243fe909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture for training\n",
    "\n",
    "# convolution block with BatchNormalization\n",
    "def ConvBlock(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(out_channels),\n",
    "             nn.ReLU(inplace=True)]\n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(4))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# resnet architecture \n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_diseases):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64)\n",
    "        self.conv2 = ConvBlock(64, 128, pool=True) # out_dim : 128 x 64 x 64 \n",
    "        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n",
    "        \n",
    "        self.conv3 = ConvBlock(128, 256, pool=True) # out_dim : 256 x 16 x 16\n",
    "        self.conv4 = ConvBlock(256, 512, pool=True) # out_dim : 512 x 4 x 44\n",
    "        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                       nn.Flatten(),\n",
    "                                       nn.Linear(512, num_diseases))\n",
    "        \n",
    "    def forward(self, xb): # xb is the loaded batch\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3a110fe-7c62-4f0b-a695-f5596d528715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet9(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=512, out_features=19, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining the model and moving it to the GPU\n",
    "model = to_device(ResNet9(3, len(train.classes)), device) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d871fae-bc57-48a3-89fb-cc5b8f424a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4        [-1, 128, 256, 256]          73,856\n",
      "       BatchNorm2d-5        [-1, 128, 256, 256]             256\n",
      "              ReLU-6        [-1, 128, 256, 256]               0\n",
      "         MaxPool2d-7          [-1, 128, 64, 64]               0\n",
      "            Conv2d-8          [-1, 128, 64, 64]         147,584\n",
      "       BatchNorm2d-9          [-1, 128, 64, 64]             256\n",
      "             ReLU-10          [-1, 128, 64, 64]               0\n",
      "           Conv2d-11          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-12          [-1, 128, 64, 64]             256\n",
      "             ReLU-13          [-1, 128, 64, 64]               0\n",
      "           Conv2d-14          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-15          [-1, 256, 64, 64]             512\n",
      "             ReLU-16          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-17          [-1, 256, 16, 16]               0\n",
      "           Conv2d-18          [-1, 512, 16, 16]       1,180,160\n",
      "      BatchNorm2d-19          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-20          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-24            [-1, 512, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-27            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-28            [-1, 512, 1, 1]               0\n",
      "          Flatten-29                  [-1, 512]               0\n",
      "           Linear-30                   [-1, 19]           9,747\n",
      "================================================================\n",
      "Total params: 6,579,987\n",
      "Trainable params: 6,579,987\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 343.95\n",
      "Params size (MB): 25.10\n",
      "Estimated Total Size (MB): 369.80\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Define INPUT_SHAPE correctly for PyTorch (Batch size is not needed)\n",
    "INPUT_SHAPE = (3, 256, 256)\n",
    "\n",
    "# Print model summary (Ensure model is defined and moved to CPU)\n",
    "print(summary(model, INPUT_SHAPE))  # ‚úÖ Correct format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "720ea0c9-0a73-44e7-b14d-ee06528f56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "def fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,\n",
    "                grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # scheduler for one cycle learniing rate\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "                \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # recording and updating learning rates\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "            \n",
    "    \n",
    "        # validation\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        \n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ea99f80-3921-4a4e-a3e3-5ae4f79d6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = r\"C:\\Users\\parvathi143\\Desktop\\webapp\\webapp\\Dataset_collection\\New Plant Diseases Dataset(Augmented)\"\n",
    "test = ImageFolder(test_dir, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e6d4e90-a7cf-44e8-bec6-d164db4b2b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple___Apple_scab',\n",
       " 'Apple___Black_rot',\n",
       " 'Apple___Cedar_apple_rust',\n",
       " 'Apple___healthy',\n",
       " 'Blueberry___healthy',\n",
       " 'Cherry_(including_sour)___Powdery_mildew',\n",
       " 'Cherry_(including_sour)___healthy',\n",
       " 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
       " 'Corn_(maize)___Common_rust_',\n",
       " 'Corn_(maize)___Northern_Leaf_Blight',\n",
       " 'Corn_(maize)___healthy',\n",
       " 'Grape___Black_rot',\n",
       " 'Grape___Esca_(Black_Measles)',\n",
       " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n",
       " 'Grape___healthy',\n",
       " 'Orange___Haunglongbing_(Citrus_greening)',\n",
       " 'Peach___Bacterial_spot',\n",
       " 'Peach___healthy',\n",
       " 'Pepper,_bell___Bacterial_spot']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = sorted(os.listdir(test_dir + '/valid')) # since images in test folder are in alphabetical order\n",
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8168a206-a190-422b-ac77-35712778b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    \"\"\"Converts image to array and return the predicted class\n",
    "        with highest probability\"\"\"\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "\n",
    "    return train.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bc23c46-6feb-48ce-aeac-acd20f24f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, label):\n",
    "    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n",
    "    plt.imshow(image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2eefc-3594-4a62-a395-1aaef3b40099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a84c8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a717533d-7006-42e8-b057-9c357b783631",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"C:\\Users\\parvathi143\\Desktop\\webapp\\webapp\\Dataset_collection\\New Plant Diseases Dataset(Augmented)\\train\"\n",
    "valid_dir = r\"C:\\Users\\parvathi143\\Desktop\\webapp\\webapp\\Dataset_collection\\New Plant Diseases Dataset(Augmented)\\valid\"\n",
    "\n",
    "# Image size\n",
    "img_size = (150, 150)\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0251b0f5-cffd-4a25-9de7-69260711225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35123 images belonging to 19 classes.\n",
      "Found 35123 images belonging to 19 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for better training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalize pixel values\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Load dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Multi-class classification\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)  # Get number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb664788-c81f-4ea1-b15c-e153780d676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parvathi143\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),  # Reduce overfitting\n",
    "    Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df66a8d0-5f44-4c85-91ca-1d199981d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parvathi143\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36064s\u001b[0m 33s/step - accuracy: 0.3814 - loss: 1.9227 - val_accuracy: 0.7422 - val_loss: 0.7381\n",
      "Epoch 2/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10427s\u001b[0m 10s/step - accuracy: 0.7331 - loss: 0.8007 - val_accuracy: 0.8510 - val_loss: 0.4318\n",
      "Epoch 3/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10929s\u001b[0m 10s/step - accuracy: 0.8015 - loss: 0.5961 - val_accuracy: 0.9033 - val_loss: 0.2817\n",
      "Epoch 4/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5523s\u001b[0m 5s/step - accuracy: 0.8463 - loss: 0.4616 - val_accuracy: 0.9042 - val_loss: 0.2950\n",
      "Epoch 5/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5032s\u001b[0m 5s/step - accuracy: 0.8758 - loss: 0.3818 - val_accuracy: 0.9464 - val_loss: 0.1578\n",
      "Epoch 6/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4577s\u001b[0m 4s/step - accuracy: 0.8897 - loss: 0.3428 - val_accuracy: 0.9277 - val_loss: 0.2268\n",
      "Epoch 7/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4356s\u001b[0m 4s/step - accuracy: 0.9024 - loss: 0.2994 - val_accuracy: 0.9554 - val_loss: 0.1311\n",
      "Epoch 8/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4259s\u001b[0m 4s/step - accuracy: 0.9151 - loss: 0.2605 - val_accuracy: 0.9494 - val_loss: 0.1537\n",
      "Epoch 9/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11986s\u001b[0m 11s/step - accuracy: 0.9167 - loss: 0.2592 - val_accuracy: 0.9462 - val_loss: 0.1688\n",
      "Epoch 10/10\n",
      "\u001b[1m1098/1098\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5118s\u001b[0m 5s/step - accuracy: 0.9303 - loss: 0.2226 - val_accuracy: 0.9478 - val_loss: 0.1737\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4d66889-382c-4a2d-8cf6-7712820a6ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved as leaf_disease.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(\"leaf_disease.h5\")\n",
    "print(\"‚úÖ Model saved as leaf_disease.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fa282-2af1-4375-a18a-3169cf9e9a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
